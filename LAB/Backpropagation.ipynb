{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Backpropagation.ipynb","provenance":[],"mount_file_id":"1TIIoX5C2puaIPZEmIzl-pMhWWp-zkbCp","authorship_tag":"ABX9TyPE8FNdlzGuiO7tn8CYdXtE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GKGxlu_jFEI","executionInfo":{"status":"ok","timestamp":1623919984632,"user_tz":-330,"elapsed":888,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}},"outputId":"561efa23-4c4e-4ee8-c3a2-2989ded7b5e8"},"source":["15.26,14.84,0.871,5.763,3.312,2.221,5.22,1\n","14.88,14.57,0.8811,5.554,3.333,1.018,4.956,1\n","14.29,14.09,0.905,5.291,3.337,2.699,4.825,1\n","13.84,13.94,0.8955,5.324,3.379,2.259,4.805,1\n","16.14,14.99,0.9034,5.658,3.562,1.355,5.175,1"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16.14, 14.99, 0.9034, 5.658, 3.562, 1.355, 5.175, 1)"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"s2F0lqpIjNm4"},"source":["1. Initialize Network\n","Let’s start with something easy, the creation of a new network ready for training.\n","\n","Each neuron has a set of weights that need to be maintained. One weight for each input connection and an additional weight for the bias. We will need to store additional properties for a neuron during training, therefore we will use a dictionary to represent each neuron and store properties by names such as ‘weights‘ for the weights.\n","\n","A network is organized into layers. The input layer is really just a row from our training dataset. The first real layer is the hidden layer. This is followed by the output layer that has one neuron for each class value.\n","\n","We will organize layers as arrays of dictionaries and treat the whole network as an array of layers.\n","\n","It is good practice to initialize the network weights to small random numbers. In this case, will we use random numbers in the range of 0 to 1.\n","\n","Below is a function named initialize_network() that creates a new neural network ready for training. It accepts three parameters, the number of inputs, the number of neurons to have in the hidden layer and the number of outputs.\n","\n","You can see that for the hidden layer we create n_hidden neurons and each neuron in the hidden layer has n_inputs + 1 weights, one for each input column in a dataset and an additional one for the bias.\n","\n","You can also see that the output layer that connects to the hidden layer has n_outputs neurons, each with n_hidden + 1 weights. This means that each neuron in the output layer connects to (has a weight for) each neuron in the hidden layer."]},{"cell_type":"code","metadata":{"id":"F2nPvFYSjiI5","executionInfo":{"status":"ok","timestamp":1623919987185,"user_tz":-330,"elapsed":1427,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","\tnetwork = list()\n","\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","\tnetwork.append(hidden_layer)\n","\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","\tnetwork.append(output_layer)\n","\treturn network"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKT2ntjCjtEm","executionInfo":{"status":"ok","timestamp":1623919987189,"user_tz":-330,"elapsed":51,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}},"outputId":"647dc391-d1b2-42ce-d409-48b946aa22cd"},"source":["from random import seed\n","from random import random\n"," \n","# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","\tnetwork = list()\n","\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","\tnetwork.append(hidden_layer)\n","\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","\tnetwork.append(output_layer)\n","\treturn network\n"," \n","seed(1)\n","network = initialize_network(2, 1, 2)\n","for layer in network:\n","\tprint(layer)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n","[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RueSZaa-jmt0"},"source":["Neuron Activation\n","The first step is to calculate the activation of one neuron given an input.\n","\n","The input could be a row from our training dataset, as in the case of the hidden layer. It may also be the outputs from each neuron in the hidden layer, in the case of the output layer.\n","\n","Neuron activation is calculated as the weighted sum of the inputs. Much like linear regression"]},{"cell_type":"code","metadata":{"id":"ZVXiqfaIk2TM","executionInfo":{"status":"ok","timestamp":1623919987191,"user_tz":-330,"elapsed":48,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","\tactivation = weights[-1]\n","\tfor i in range(len(weights)-1):\n","\t\tactivation += weights[i] * inputs[i]\n","\treturn activation"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yji8avGajwPc"},"source":["Neuron Transfer\n","Once a neuron is activated, we need to transfer the activation to see what the neuron output actually is.\n","\n","Different transfer functions can be used. It is traditional to use the sigmoid activation function, but you can also use the tanh (hyperbolic tangent) function to transfer outputs. More recently, the rectifier transfer function has been popular with large deep learning networks.\n","\n","The sigmoid activation function looks like an S shape, it’s also called the logistic function. It can take any input value and produce a number between 0 and 1 on an S-curve. It is also a function of which we can easily calculate the derivative (slope) that we will need later when backpropagating error."]},{"cell_type":"code","metadata":{"id":"Pj4NJbFfk_6A","executionInfo":{"status":"ok","timestamp":1623919987193,"user_tz":-330,"elapsed":49,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Transfer neuron activation\n","def transfer(activation):\n","\treturn 1.0 / (1.0 + exp(-activation))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ya8pYgRej_UV"},"source":["Forward Propagation\n","Forward propagating an input is straightforward.\n","\n","We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer.\n","\n","Below is a function named forward_propagate() that implements the forward propagation for a row of data from our dataset with our neural network.\n","\n","You can see that a neuron’s output value is stored in the neuron with the name ‘output‘. You can also see that we collect the outputs for a layer in an array named new_inputs that becomes the array inputs and is used as inputs for the following layer.\n","\n","The function returns the outputs from the last layer also called the output layer."]},{"cell_type":"code","metadata":{"id":"srDWxqewmEgN","executionInfo":{"status":"ok","timestamp":1623919987197,"user_tz":-330,"elapsed":50,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","\tinputs = row\n","\tfor layer in network:\n","\t\tnew_inputs = []\n","\t\tfor neuron in layer:\n","\t\t\tactivation = activate(neuron['weights'], inputs)\n","\t\t\tneuron['output'] = transfer(activation)\n","\t\t\tnew_inputs.append(neuron['output'])\n","\t\tinputs = new_inputs\n","\treturn inputs"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KgCIID7mYj-","executionInfo":{"status":"ok","timestamp":1623919987198,"user_tz":-330,"elapsed":49,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}},"outputId":"483f2b27-1ccb-4331-a38b-d5144df5475a"},"source":["from math import exp\n"," \n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","\tactivation = weights[-1]\n","\tfor i in range(len(weights)-1):\n","\t\tactivation += weights[i] * inputs[i]\n","\treturn activation\n"," \n","# Transfer neuron activation\n","def transfer(activation):\n","\treturn 1.0 / (1.0 + exp(-activation))\n"," \n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","\tinputs = row\n","\tfor layer in network:\n","\t\tnew_inputs = []\n","\t\tfor neuron in layer:\n","\t\t\tactivation = activate(neuron['weights'], inputs)\n","\t\t\tneuron['output'] = transfer(activation)\n","\t\t\tnew_inputs.append(neuron['output'])\n","\t\tinputs = new_inputs\n","\treturn inputs\n"," \n","# test forward propagation\n","network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n","\t\t[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n","row = [1, 0, None]\n","output = forward_propagate(network, row)\n","print(output)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[0.6629970129852887, 0.7253160725279748]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7t5kIMiPke-x"},"source":["Back Propagate Error\n","The backpropagation algorithm is named for the way in which weights are trained.\n","\n","Error is calculated between the expected outputs and the outputs forward propagated from the network. These errors are then propagated backward through the network from the output layer to the hidden layer, assigning blame for the error and updating weights as they go.\n","\n","The math for backpropagating error is rooted in calculus, but we will remain high level in this section and focus on what is calculated and how rather than why the calculations take this particular form.\n","\n","This part is broken down into two sections.\n","\n","Transfer Derivative.\n","Error Backpropagation.\n","3.1. Transfer Derivative\n","Given an output value from a neuron, we need to calculate it’s slope."]},{"cell_type":"code","metadata":{"id":"VnUaTD3Dm3El","executionInfo":{"status":"ok","timestamp":1623919987200,"user_tz":-330,"elapsed":47,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","\treturn output * (1.0 - output)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2Ij2XT8lK-I"},"source":["The first step is to calculate the error for each output neuron, this will give us our error signal (input) to propagate backwards through the network.\n","\n","The error for a given neuron can be calculated as follows:\n","\n","error = (expected - output) * transfer_derivative(output)\n","1\n","error = (expected - output) * transfer_derivative(output)\n","Where expected is the expected output value for the neuron, output is the output value for the neuron and transfer_derivative() calculates the slope of the neuron’s output value, as shown above.\n","\n","This error calculation is used for neurons in the output layer. The expected value is the class value itself. In the hidden layer, things are a little more complicated.\n","\n","The error signal for a neuron in the hidden layer is calculated as the weighted error of each neuron in the output layer. Think of the error traveling back along the weights of the output layer to the neurons in the hidden layer.\n","\n","The back-propagated error signal is accumulated and then used to determine the error for the neuron in the hidden layer, as follows:\n","\n","1\n","error = (weight_k * error_j) * transfer_derivative(output)\n","Where error_j is the error signal from the jth neuron in the output layer, weight_k is the weight that connects the kth neuron to the current neuron and output is the output for the current neuron.\n","\n","Below is a function named backward_propagate_error() that implements this procedure.\n","\n","You can see that the error signal calculated for each neuron is stored with the name ‘delta’. You can see that the layers of the network are iterated in reverse order, starting at the output and working backwards. This ensures that the neurons in the output layer have ‘delta’ values calculated first that neurons in the hidden layer can use in the subsequent iteration. I chose the name ‘delta’ to reflect the change the error implies on the neuron (e.g. the weight delta).\n","\n","You can see that the error signal for neurons in the hidden layer is accumulated from neurons in the output layer where the hidden neuron number j is also the index of the neuron’s weight in the output layer neuron[‘weights’][j]."]},{"cell_type":"code","metadata":{"id":"jh7xl-FDnFbO","executionInfo":{"status":"ok","timestamp":1623919987202,"user_tz":-330,"elapsed":48,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","\tfor i in reversed(range(len(network))):\n","\t\tlayer = network[i]\n","\t\terrors = list()\n","\t\tif i != len(network)-1:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\terror = 0.0\n","\t\t\t\tfor neuron in network[i + 1]:\n","\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n","\t\t\t\terrors.append(error)\n","\t\telse:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\tneuron = layer[j]\n","\t\t\t\terrors.append(expected[j] - neuron['output'])\n","\t\tfor j in range(len(layer)):\n","\t\t\tneuron = layer[j]\n","\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abaVntXDnHoM","executionInfo":{"status":"ok","timestamp":1623919987203,"user_tz":-330,"elapsed":48,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}},"outputId":"c63a4608-f6ea-4c0a-b8fd-a59440432f8d"},"source":["# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","\treturn output * (1.0 - output)\n"," \n","# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","\tfor i in reversed(range(len(network))):\n","\t\tlayer = network[i]\n","\t\terrors = list()\n","\t\tif i != len(network)-1:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\terror = 0.0\n","\t\t\t\tfor neuron in network[i + 1]:\n","\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n","\t\t\t\terrors.append(error)\n","\t\telse:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\tneuron = layer[j]\n","\t\t\t\terrors.append(expected[j] - neuron['output'])\n","\t\tfor j in range(len(layer)):\n","\t\t\tneuron = layer[j]\n","\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n"," \n","# test backpropagation of error\n","network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n","\t\t[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n","expected = [0, 1]\n","backward_propagate_error(network, expected)\n","for layer in network:\n","\tprint(layer)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n","[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zgErI71xleoI"},"source":["Train Network\n","The network is trained using stochastic gradient descent.\n","\n","This involves multiple iterations of exposing a training dataset to the network and for each row of data forward propagating the inputs, backpropagating the error and updating the network weights.\n","\n","This part is broken down into two sections:\n","\n","Update Weights.\n","Train Network.\n","4.1. Update Weights\n","Once errors are calculated for each neuron in the network via the back propagation method above, they can be used to update weights.\n","\n","Network weights are updated as follows:"]},{"cell_type":"code","metadata":{"id":"Q1vnRXG7nWlx","executionInfo":{"status":"ok","timestamp":1623919987203,"user_tz":-330,"elapsed":44,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Update network weights with error\n","def update_weights(network, row, l_rate):\n","\tfor i in range(len(network)):\n","\t\tinputs = row[:-1]\n","\t\tif i != 0:\n","\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n","\t\tfor neuron in network[i]:\n","\t\t\tfor j in range(len(inputs)):\n","\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n","\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0dOvbRwlj_M"},"source":["Train Network\n","As mentioned, the network is updated using stochastic gradient descent.\n","\n","This involves first looping for a fixed number of epochs and within each epoch updating the network for each row in the training dataset.\n","\n","Because updates are made for each training pattern, this type of learning is called online learning. If errors were accumulated across an epoch before updating the weights, this is called batch learning or batch gradient descent.\n","\n","Below is a function that implements the training of an already initialized neural network with a given training dataset, learning rate, fixed number of epochs and an expected number of output values.\n","\n","The expected number of output values is used to transform class values in the training data into a one hot encoding. That is a binary vector with one column for each class value to match the output of the network. This is required to calculate the error for the output layer.\n","\n","You can also see that the sum squared error between the expected output and the network output is accumulated each epoch and printed. This is helpful to create a trace of how much the network is learning and improving each epoch."]},{"cell_type":"code","metadata":{"id":"cUzhaBtQnbyM","executionInfo":{"status":"ok","timestamp":1623919987205,"user_tz":-330,"elapsed":42,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","\tfor epoch in range(n_epoch):\n","\t\tsum_error = 0\n","\t\tfor row in train:\n","\t\t\toutputs = forward_propagate(network, row)\n","\t\t\texpected = [0 for i in range(n_outputs)]\n","\t\t\texpected[row[-1]] = 1\n","\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","\t\t\tbackward_propagate_error(network, expected)\n","\t\t\tupdate_weights(network, row, l_rate)\n","\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":129},"id":"MnudupuUngKm","executionInfo":{"status":"error","timestamp":1623919987211,"user_tz":-330,"elapsed":47,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}},"outputId":"75734c84-673a-46ec-f10e-86248cbc7fa8"},"source":["X1\t\t       \tX2\t\t\t     Y\n","2.7810836\t\t2.550537003\t\t0\n","1.465489372\t\t2.362125076\t\t0\n","3.396561688\t\t4.400293529\t\t0\n","1.38807019\t\t1.850220317\t\t0\n","3.06407232\t\t3.005305973\t\t0\n","7.627531214\t\t2.759262235\t\t1\n","5.332441248\t\t2.088626775\t\t1\n","6.922596716\t\t1.77106367\t\t1\n","8.675418651\t\t-0.242068655\t\t1\n","7.673756466\t\t3.508563011\t\t1"],"execution_count":13,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-483d70312635>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    X1\t\t       \tX2\t\t\t     Y\u001b[0m\n\u001b[0m      \t\t       \t ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"9r2zAwa7nuq_","executionInfo":{"status":"aborted","timestamp":1623919987206,"user_tz":-330,"elapsed":36,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["from math import exp\n","from random import seed\n","from random import random\n"," \n","# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","\tnetwork = list()\n","\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","\tnetwork.append(hidden_layer)\n","\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","\tnetwork.append(output_layer)\n","\treturn network\n"," \n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","\tactivation = weights[-1]\n","\tfor i in range(len(weights)-1):\n","\t\tactivation += weights[i] * inputs[i]\n","\treturn activation\n"," \n","# Transfer neuron activation\n","def transfer(activation):\n","\treturn 1.0 / (1.0 + exp(-activation))\n"," \n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","\tinputs = row\n","\tfor layer in network:\n","\t\tnew_inputs = []\n","\t\tfor neuron in layer:\n","\t\t\tactivation = activate(neuron['weights'], inputs)\n","\t\t\tneuron['output'] = transfer(activation)\n","\t\t\tnew_inputs.append(neuron['output'])\n","\t\tinputs = new_inputs\n","\treturn inputs\n"," \n","# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","\treturn output * (1.0 - output)\n"," \n","# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","\tfor i in reversed(range(len(network))):\n","\t\tlayer = network[i]\n","\t\terrors = list()\n","\t\tif i != len(network)-1:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\terror = 0.0\n","\t\t\t\tfor neuron in network[i + 1]:\n","\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n","\t\t\t\terrors.append(error)\n","\t\telse:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\tneuron = layer[j]\n","\t\t\t\terrors.append(expected[j] - neuron['output'])\n","\t\tfor j in range(len(layer)):\n","\t\t\tneuron = layer[j]\n","\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n"," \n","# Update network weights with error\n","def update_weights(network, row, l_rate):\n","\tfor i in range(len(network)):\n","\t\tinputs = row[:-1]\n","\t\tif i != 0:\n","\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n","\t\tfor neuron in network[i]:\n","\t\t\tfor j in range(len(inputs)):\n","\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n","\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n"," \n","# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","\tfor epoch in range(n_epoch):\n","\t\tsum_error = 0\n","\t\tfor row in train:\n","\t\t\toutputs = forward_propagate(network, row)\n","\t\t\texpected = [0 for i in range(n_outputs)]\n","\t\t\texpected[row[-1]] = 1\n","\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","\t\t\tbackward_propagate_error(network, expected)\n","\t\t\tupdate_weights(network, row, l_rate)\n","\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n"," \n","# Test training backprop algorithm\n","seed(1)\n","dataset = [[2.7810836,2.550537003,0],\n","\t[1.465489372,2.362125076,0],\n","\t[3.396561688,4.400293529,0],\n","\t[1.38807019,1.850220317,0],\n","\t[3.06407232,3.005305973,0],\n","\t[7.627531214,2.759262235,1],\n","\t[5.332441248,2.088626775,1],\n","\t[6.922596716,1.77106367,1],\n","\t[8.675418651,-0.242068655,1],\n","\t[7.673756466,3.508563011,1]]\n","n_inputs = len(dataset[0]) - 1\n","n_outputs = len(set([row[-1] for row in dataset]))\n","network = initialize_network(n_inputs, 2, n_outputs)\n","train_network(network, dataset, 0.5, 20, n_outputs)\n","for layer in network:\n","\tprint(layer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_bdmRJOl5Lu"},"source":["Predict\n","Making predictions with a trained neural network is easy enough.\n","\n","We have already seen how to forward-propagate an input pattern to get an output. This is all we need to do to make a prediction. We can use the output values themselves directly as the probability of a pattern belonging to each output class.\n","\n","It may be more useful to turn this output back into a crisp class prediction. We can do this by selecting the class value with the larger probability. This is also called the arg max function.\n","\n","Below is a function named predict() that implements this procedure. It returns the index in the network output that has the largest probability. It assumes that class values have been converted to integers starting at 0."]},{"cell_type":"code","metadata":{"id":"A-NEBoQ6n7AF","executionInfo":{"status":"aborted","timestamp":1623919987207,"user_tz":-330,"elapsed":37,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Make a prediction with a network\n","def predict(network, row):\n","\toutputs = forward_propagate(network, row)\n","\treturn outputs.index(max(outputs))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hbQJanRoC3I","executionInfo":{"status":"aborted","timestamp":1623919987208,"user_tz":-330,"elapsed":37,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["from math import exp\n"," \n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","\tactivation = weights[-1]\n","\tfor i in range(len(weights)-1):\n","\t\tactivation += weights[i] * inputs[i]\n","\treturn activation\n"," \n","# Transfer neuron activation\n","def transfer(activation):\n","\treturn 1.0 / (1.0 + exp(-activation))\n"," \n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","\tinputs = row\n","\tfor layer in network:\n","\t\tnew_inputs = []\n","\t\tfor neuron in layer:\n","\t\t\tactivation = activate(neuron['weights'], inputs)\n","\t\t\tneuron['output'] = transfer(activation)\n","\t\t\tnew_inputs.append(neuron['output'])\n","\t\tinputs = new_inputs\n","\treturn inputs\n"," \n","# Make a prediction with a network\n","def predict(network, row):\n","\toutputs = forward_propagate(network, row)\n","\treturn outputs.index(max(outputs))\n"," \n","# Test making predictions with the network\n","dataset = [[2.7810836,2.550537003,0],\n","\t[1.465489372,2.362125076,0],\n","\t[3.396561688,4.400293529,0],\n","\t[1.38807019,1.850220317,0],\n","\t[3.06407232,3.005305973,0],\n","\t[7.627531214,2.759262235,1],\n","\t[5.332441248,2.088626775,1],\n","\t[6.922596716,1.77106367,1],\n","\t[8.675418651,-0.242068655,1],\n","\t[7.673756466,3.508563011,1]]\n","network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n","\t[{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\n","for row in dataset:\n","\tprediction = predict(network, row)\n","\tprint('Expected=%d, Got=%d' % (row[-1], prediction))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vGHIxMDmBls"},"source":["Wheat Seeds Dataset\n","This section applies the Backpropagation algorithm to the wheat seeds dataset.\n","\n","The first step is to load the dataset and convert the loaded data to numbers that we can use in our neural network. For this we will use the helper function load_csv() to load the file, str_column_to_float() to convert string numbers to floats and str_column_to_int() to convert the class column to integer values.\n","\n","Input values vary in scale and need to be normalized to the range of 0 and 1. It is generally good practice to normalize input values to the range of the chosen transfer function, in this case, the sigmoid function that outputs values between 0 and 1. The dataset_minmax() and normalize_dataset() helper functions were used to normalize the input values.\n","\n","We will evaluate the algorithm using k-fold cross-validation with 5 folds. This means that 201/5=40.2 or 40 records will be in each fold. We will use the helper functions evaluate_algorithm() to evaluate the algorithm with cross-validation and accuracy_metric() to calculate the accuracy of predictions.\n","\n","A new function named back_propagation() was developed to manage the application of the Backpropagation algorithm, first initializing a network, training it on the training dataset and then using the trained network to make predictions on a test dataset."]},{"cell_type":"code","metadata":{"id":"L4uZeJQzoLrd","executionInfo":{"status":"aborted","timestamp":1623919987209,"user_tz":-330,"elapsed":38,"user":{"displayName":"20WH1DB008 VALUGUBELLY ANUPAMA","photoUrl":"","userId":"02251695447687599737"}}},"source":["# Backprop on the Seeds Dataset\n","from random import seed\n","from random import randrange\n","from random import random\n","from csv import reader\n","from math import exp\n"," \n","# Load a CSV file\n","def load_csv(filename):\n","\tdataset = list()\n","\twith open(\"/content/drive/MyDrive/wheat-seeds (1).txt\", 'r') as file:\n","\t\tcsv_reader = reader(file)\n","\t\tfor row in csv_reader:\n","\t\t\tif not row:\n","\t\t\t\tcontinue\n","\t\t\tdataset.append(row)\n","\treturn dataset\n"," \n","# Convert string column to float\n","def str_column_to_float(dataset, column):\n","\tfor row in dataset:\n","\t\trow[column] = float(row[column].strip())\n"," \n","# Convert string column to integer\n","def str_column_to_int(dataset, column):\n","\tclass_values = [row[column] for row in dataset]\n","\tunique = set(class_values)\n","\tlookup = dict()\n","\tfor i, value in enumerate(unique):\n","\t\tlookup[value] = i\n","\tfor row in dataset:\n","\t\trow[column] = lookup[row[column]]\n","\treturn lookup\n"," \n","# Find the min and max values for each column\n","def dataset_minmax(dataset):\n","\tminmax = list()\n","\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n","\treturn stats\n"," \n","# Rescale dataset columns to the range 0-1\n","def normalize_dataset(dataset, minmax):\n","\tfor row in dataset:\n","\t\tfor i in range(len(row)-1):\n","\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n"," \n","# Split a dataset into k folds\n","def cross_validation_split(dataset, n_folds):\n","\tdataset_split = list()\n","\tdataset_copy = list(dataset)\n","\tfold_size = int(len(dataset) / n_folds)\n","\tfor i in range(n_folds):\n","\t\tfold = list()\n","\t\twhile len(fold) < fold_size:\n","\t\t\tindex = randrange(len(dataset_copy))\n","\t\t\tfold.append(dataset_copy.pop(index))\n","\t\tdataset_split.append(fold)\n","\treturn dataset_split\n"," \n","# Calculate accuracy percentage\n","def accuracy_metric(actual, predicted):\n","\tcorrect = 0\n","\tfor i in range(len(actual)):\n","\t\tif actual[i] == predicted[i]:\n","\t\t\tcorrect += 1\n","\treturn correct / float(len(actual)) * 100.0\n"," \n","# Evaluate an algorithm using a cross validation split\n","def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n","\tfolds = cross_validation_split(dataset, n_folds)\n","\tscores = list()\n","\tfor fold in folds:\n","\t\ttrain_set = list(folds)\n","\t\ttrain_set.remove(fold)\n","\t\ttrain_set = sum(train_set, [])\n","\t\ttest_set = list()\n","\t\tfor row in fold:\n","\t\t\trow_copy = list(row)\n","\t\t\ttest_set.append(row_copy)\n","\t\t\trow_copy[-1] = None\n","\t\tpredicted = algorithm(train_set, test_set, *args)\n","\t\tactual = [row[-1] for row in fold]\n","\t\taccuracy = accuracy_metric(actual, predicted)\n","\t\tscores.append(accuracy)\n","\treturn scores\n"," \n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","\tactivation = weights[-1]\n","\tfor i in range(len(weights)-1):\n","\t\tactivation += weights[i] * inputs[i]\n","\treturn activation\n"," \n","# Transfer neuron activation\n","def transfer(activation):\n","\treturn 1.0 / (1.0 + exp(-activation))\n"," \n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","\tinputs = row\n","\tfor layer in network:\n","\t\tnew_inputs = []\n","\t\tfor neuron in layer:\n","\t\t\tactivation = activate(neuron['weights'], inputs)\n","\t\t\tneuron['output'] = transfer(activation)\n","\t\t\tnew_inputs.append(neuron['output'])\n","\t\tinputs = new_inputs\n","\treturn inputs\n"," \n","# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","\treturn output * (1.0 - output)\n"," \n","# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","\tfor i in reversed(range(len(network))):\n","\t\tlayer = network[i]\n","\t\terrors = list()\n","\t\tif i != len(network)-1:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\terror = 0.0\n","\t\t\t\tfor neuron in network[i + 1]:\n","\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n","\t\t\t\terrors.append(error)\n","\t\telse:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\tneuron = layer[j]\n","\t\t\t\terrors.append(expected[j] - neuron['output'])\n","\t\tfor j in range(len(layer)):\n","\t\t\tneuron = layer[j]\n","\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n"," \n","# Update network weights with error\n","def update_weights(network, row, l_rate):\n","\tfor i in range(len(network)):\n","\t\tinputs = row[:-1]\n","\t\tif i != 0:\n","\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n","\t\tfor neuron in network[i]:\n","\t\t\tfor j in range(len(inputs)):\n","\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n","\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n"," \n","# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","\tfor epoch in range(n_epoch):\n","\t\tfor row in train:\n","\t\t\toutputs = forward_propagate(network, row)\n","\t\t\texpected = [0 for i in range(n_outputs)]\n","\t\t\texpected[row[-1]] = 1\n","\t\t\tbackward_propagate_error(network, expected)\n","\t\t\tupdate_weights(network, row, l_rate)\n"," \n","# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","\tnetwork = list()\n","\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","\tnetwork.append(hidden_layer)\n","\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","\tnetwork.append(output_layer)\n","\treturn network\n"," \n","# Make a prediction with a network\n","def predict(network, row):\n","\toutputs = forward_propagate(network, row)\n","\treturn outputs.index(max(outputs))\n"," \n","# Backpropagation Algorithm With Stochastic Gradient Descent\n","def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n","\tn_inputs = len(train[0]) - 1\n","\tn_outputs = len(set([row[-1] for row in train]))\n","\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n","\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n","\tpredictions = list()\n","\tfor row in test:\n","\t\tprediction = predict(network, row)\n","\t\tpredictions.append(prediction)\n","\treturn(predictions)\n"," \n","# Test Backprop on Seeds dataset\n","seed(1)\n","# load and prepare data\n","filename = 'seeds_dataset.csv'\n","dataset = load_csv(filename)\n","for i in range(len(dataset[0])-1):\n","\tstr_column_to_float(dataset, i)\n","# convert class column to integers\n","str_column_to_int(dataset, len(dataset[0])-1)\n","# normalize input variables\n","minmax = dataset_minmax(dataset)\n","normalize_dataset(dataset, minmax)\n","# evaluate algorithm\n","n_folds = 5\n","l_rate = 0.3\n","n_epoch = 500\n","n_hidden = 5\n","scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n","print('Scores: %s' % scores)\n","print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"],"execution_count":null,"outputs":[]}]}